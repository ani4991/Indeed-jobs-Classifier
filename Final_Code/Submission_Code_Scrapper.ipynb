{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re, urllib\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import csv, time, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the input company name file\n",
    "\n",
    "comapny_df = pd.read_csv(\"C:\\\\Users\\\\ani49\\\\Data_Science_Case_Studies\\\\Enlyft\\\\enlyft_datascience_sample.csv\")\n",
    "company_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting to suitable list format\n",
    "\n",
    "comapny_list = comapny_df['Company Name'].values.tolist()\n",
    "len(comapny_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## web scrapper function - for all the companies in the list, this function scrapes 4 features:\n",
    "\n",
    "# 1. Job title\n",
    "# 2. Company\n",
    "# 3. Location\n",
    "# 4. Summary\n",
    "\n",
    "for i in companies_list:\n",
    "    \n",
    "    #name = i.split()\n",
    "    #query = \"\"\n",
    "    page = 0\n",
    "    with open('C:\\\\Users\\\\ani49\\\\Data_Science_Case_Studies\\\\Enlyft\\\\scrape_results-2.csv', 'a', encoding='utf-8', newline='') as f_output:\n",
    "        csv_print = csv.writer(f_output)\n",
    "\n",
    "        file_is_empty = os.stat('C:\\\\Users\\\\ani49\\\\Data_Science_Case_Studies\\\\Enlyft\\\\scrape_results-2.csv').st_size == 0\n",
    "        if file_is_empty:\n",
    "            csv_print.writerow(['Job_title', 'Company', 'Location', 'Summary'])\n",
    "\n",
    "        #for j in range(len(name)-1):\n",
    "            #query += name[j] + '+'\n",
    "\n",
    "        #query += name[len(name)-1]\n",
    "        \n",
    "        URL = 'https://www.indeed.com/jobs?q={}&start={}'.format(urllib.parse.quote_plus(i),page)\n",
    "        print(\"URL connected is: \", URL)\n",
    "        request = requests.get(URL)\n",
    "        \n",
    "        while(request.status_code == 200):\n",
    "\n",
    "            soup = BeautifulSoup(urllib.request.urlopen(URL).read(), 'html.parser')\n",
    "\n",
    "\n",
    "            for jobs in soup.find_all(class_='result'):\n",
    "\n",
    "                try:\n",
    "                    title = jobs.div.text.strip()\n",
    "                except Exception as e:\n",
    "                    title = None\n",
    "                #print('Job_title:', title)\n",
    "\n",
    "\n",
    "                try:\n",
    "                    company = jobs.span.text.strip()\n",
    "                except Exception as e:\n",
    "                    company = None\n",
    "                #print(\"Company:\", company)\n",
    "\n",
    "\n",
    "                try:\n",
    "                    location = jobs.find('span', class_='location').text.strip()\n",
    "                except Exception as e:\n",
    "                    location=None\n",
    "                #print(\"Location:\", location)\n",
    "\n",
    "\n",
    "                try:\n",
    "                    summary = jobs.find('div', class_='summary').text.strip()\n",
    "                except Exception as e:\n",
    "                    summary=None\n",
    "                #print('Summary:', summary)\n",
    "\n",
    "\n",
    "                csv_print.writerow([title, company, location, summary])\n",
    "\n",
    "                #print(\"---------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            page += 10\n",
    "            URL = 'https://www.indeed.com/jobs?q={}&start={}'.format(urllib.parse.quote_plus(i),page)\n",
    "            request = requests.get(URL)\n",
    "        \n",
    "    df = pd.read_csv('C:\\\\Users\\\\ani49\\\\Data_Science_Case_Studies\\\\Enlyft\\\\scrape_results-2.csv')\n",
    "    list_of_companies_df_dict_2[i] = df\n",
    "    os.remove('C:\\\\Users\\\\ani49\\\\Data_Science_Case_Studies\\\\Enlyft\\\\scrape_results-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finally saving the intermediate dataframe object to respective company data as csv files \n",
    "\n",
    "for key,value in list_of_companies_df_dict_2.items():\n",
    "    print(key)\n",
    "    df.to_csv('C:\\\\Users\\\\ani49\\\\Data_Science_Case_Studies\\\\Enlyft\\\\new\\\\' + key + '.csv', sep=',', index=False,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
